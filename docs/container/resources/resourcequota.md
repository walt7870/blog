# ResourceQuota

## æ¦‚è¿°

ResourceQuota æ˜¯ Kubernetes ä¸­ç”¨äºé™åˆ¶å‘½åç©ºé—´èµ„æºæ€»é‡çš„å¯¹è±¡ã€‚å®ƒå¯ä»¥é™åˆ¶å‘½åç©ºé—´ä¸­å¯ä»¥åˆ›å»ºçš„å¯¹è±¡æ•°é‡ï¼Œä»¥åŠè¿™äº›å¯¹è±¡å¯ä»¥æ¶ˆè€—çš„è®¡ç®—èµ„æºæ€»é‡ï¼Œä¸ºå¤šç§Ÿæˆ·ç¯å¢ƒæä¾›èµ„æºéš”ç¦»å’Œå…¬å¹³åˆ†é…ã€‚

| å…³é”®ç‚¹ | å†…å®¹ |
| ---- | ---- |
| æ ¸å¿ƒä½œç”¨ | é™åˆ¶å‘½åç©ºé—´å†…èµ„æºçš„æ€»ä½“ä½¿ç”¨é‡ |
| ä½œç”¨èŒƒå›´ | å‘½åç©ºé—´çº§åˆ« |
| é™åˆ¶ç±»å‹ | è®¡ç®—èµ„æºã€å­˜å‚¨èµ„æºã€å¯¹è±¡æ•°é‡ |
| æ‰§è¡Œæœºåˆ¶ | å‡†å…¥æ§åˆ¶å™¨å®æ—¶æ£€æŸ¥ |
| é…é¢çŠ¶æ€ | å®æ—¶è·Ÿè¸ªå·²ä½¿ç”¨å’Œå‰©ä½™é…é¢ |

## ResourceQuota çš„æœ¬è´¨

### è®¾è®¡ç†å¿µ

* **èµ„æºæ€»é‡æ§åˆ¶**ï¼šé™åˆ¶å‘½åç©ºé—´å†…æ‰€æœ‰èµ„æºçš„æ€»ä½“ä½¿ç”¨é‡
* **å¤šç§Ÿæˆ·éš”ç¦»**ï¼šä¸ºä¸åŒç§Ÿæˆ·æä¾›èµ„æºéš”ç¦»å’Œå…¬å¹³åˆ†é…
* **é¢„ç®—ç®¡ç†**ï¼šç±»ä¼¼äº‘è®¡ç®—çš„èµ„æºé¢„ç®—æ¦‚å¿µ
* **é˜²æ­¢èµ„æºè€—å°½**ï¼šé¿å…å•ä¸ªå‘½åç©ºé—´æ¶ˆè€—è¿‡å¤šé›†ç¾¤èµ„æº

### å·¥ä½œåŸç†

```
èµ„æºåˆ›å»ºè¯·æ±‚ â†’ ResourceQuota æ£€æŸ¥ â†’ é…é¢éªŒè¯ â†’ æ›´æ–°ä½¿ç”¨é‡ â†’ èµ„æºåˆ›å»º
       â†“              â†“              â†“           â†“           â†“
   ç”¨æˆ·æäº¤        å‡†å…¥æ§åˆ¶å™¨      æ£€æŸ¥å‰©ä½™é…é¢   æ›´æ–°è®¡æ•°å™¨    æˆåŠŸ/å¤±è´¥
```

### ResourceQuota vs LimitRange

| ç‰¹æ€§ | ResourceQuota | LimitRange |
| ---- | ---- | ---- |
| æ§åˆ¶èŒƒå›´ | å‘½åç©ºé—´æ€»é‡ | å•ä¸ªèµ„æºå¯¹è±¡ |
| é™åˆ¶ç±»å‹ | æ€»é‡é™åˆ¶ | å•ä½“é™åˆ¶ |
| å¯¹è±¡æ•°é‡ | æ”¯æŒ | ä¸æ”¯æŒ |
| é»˜è®¤å€¼ | ä¸æ”¯æŒ | æ”¯æŒ |
| ä½¿ç”¨åœºæ™¯ | å¤šç§Ÿæˆ·èµ„æºåˆ†é… | èµ„æºè§„èŒƒåŒ– |

## åŸºæœ¬é…ç½®

### 1. è®¡ç®—èµ„æºé…é¢

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: development
  labels:
    quota-type: compute
    environment: development
spec:
  hard:
    # CPU é…é¢
    requests.cpu: "10"        # CPU è¯·æ±‚æ€»é‡é™åˆ¶
    limits.cpu: "20"          # CPU é™åˆ¶æ€»é‡é™åˆ¶
    
    # å†…å­˜é…é¢
    requests.memory: 20Gi     # å†…å­˜è¯·æ±‚æ€»é‡é™åˆ¶
    limits.memory: 40Gi       # å†…å­˜é™åˆ¶æ€»é‡é™åˆ¶
    
    # ä¸´æ—¶å­˜å‚¨é…é¢
    requests.ephemeral-storage: 50Gi   # ä¸´æ—¶å­˜å‚¨è¯·æ±‚æ€»é‡
    limits.ephemeral-storage: 100Gi    # ä¸´æ—¶å­˜å‚¨é™åˆ¶æ€»é‡

---
# æµ‹è¯• Pod - æ¶ˆè€—é…é¢
apiVersion: v1
kind: Pod
metadata:
  name: quota-test-1
  namespace: development
spec:
  containers:
  - name: app
    image: nginx:1.20
    resources:
      requests:
        cpu: "1"          # æ¶ˆè€— 1 CPU è¯·æ±‚é…é¢
        memory: 2Gi       # æ¶ˆè€— 2Gi å†…å­˜è¯·æ±‚é…é¢
      limits:
        cpu: "2"          # æ¶ˆè€— 2 CPU é™åˆ¶é…é¢
        memory: 4Gi       # æ¶ˆè€— 4Gi å†…å­˜é™åˆ¶é…é¢

---
# ç¬¬äºŒä¸ª Pod
apiVersion: v1
kind: Pod
metadata:
  name: quota-test-2
  namespace: development
spec:
  containers:
  - name: app
    image: busybox:1.35
    resources:
      requests:
        cpu: "500m"       # æ¶ˆè€— 0.5 CPU è¯·æ±‚é…é¢
        memory: 1Gi       # æ¶ˆè€— 1Gi å†…å­˜è¯·æ±‚é…é¢
      limits:
        cpu: "1"          # æ¶ˆè€— 1 CPU é™åˆ¶é…é¢
        memory: 2Gi       # æ¶ˆè€— 2Gi å†…å­˜é™åˆ¶é…é¢

# å½“å‰é…é¢ä½¿ç”¨æƒ…å†µï¼š
# CPU requests: 1.5/10, limits: 3/20
# Memory requests: 3Gi/20Gi, limits: 6Gi/40Gi
```

### 2. å¯¹è±¡æ•°é‡é…é¢

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-count-quota
  namespace: production
  labels:
    quota-type: object-count
    environment: production
spec:
  hard:
    # Pod ç›¸å…³
    pods: "50"                    # æœ€å¤š 50 ä¸ª Pod
    replicationcontrollers: "10"  # æœ€å¤š 10 ä¸ª RC
    
    # å·¥ä½œè´Ÿè½½æ§åˆ¶å™¨
    deployments.apps: "20"        # æœ€å¤š 20 ä¸ª Deployment
    replicasets.apps: "30"        # æœ€å¤š 30 ä¸ª ReplicaSet
    statefulsets.apps: "5"        # æœ€å¤š 5 ä¸ª StatefulSet
    daemonsets.apps: "3"          # æœ€å¤š 3 ä¸ª DaemonSet
    jobs.batch: "10"              # æœ€å¤š 10 ä¸ª Job
    cronjobs.batch: "5"           # æœ€å¤š 5 ä¸ª CronJob
    
    # æœåŠ¡å’Œç½‘ç»œ
    services: "20"                # æœ€å¤š 20 ä¸ª Service
    services.loadbalancers: "3"   # æœ€å¤š 3 ä¸ª LoadBalancer æœåŠ¡
    services.nodeports: "5"       # æœ€å¤š 5 ä¸ª NodePort æœåŠ¡
    ingresses.networking.k8s.io: "10"  # æœ€å¤š 10 ä¸ª Ingress
    
    # é…ç½®å’Œå­˜å‚¨
    configmaps: "30"              # æœ€å¤š 30 ä¸ª ConfigMap
    secrets: "20"                 # æœ€å¤š 20 ä¸ª Secret
    persistentvolumeclaims: "15"  # æœ€å¤š 15 ä¸ª PVC
    
    # RBAC
    roles.rbac.authorization.k8s.io: "10"         # æœ€å¤š 10 ä¸ª Role
    rolebindings.rbac.authorization.k8s.io: "15"  # æœ€å¤š 15 ä¸ª RoleBinding

---
# æµ‹è¯•å¯¹è±¡åˆ›å»º
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-deployment-1
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test-app-1
  template:
    metadata:
      labels:
        app: test-app-1
    spec:
      containers:
      - name: app
        image: nginx:1.20
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi

# è¿™ä¸ª Deployment ä¼šï¼š
# - æ¶ˆè€— 1 ä¸ª deployments.apps é…é¢
# - åˆ›å»º 1 ä¸ª ReplicaSetï¼ˆæ¶ˆè€— replicasets.apps é…é¢ï¼‰
# - åˆ›å»º 3 ä¸ª Podï¼ˆæ¶ˆè€— pods é…é¢ï¼‰
```

### 3. å­˜å‚¨é…é¢

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: data-processing
  labels:
    quota-type: storage
    team: data-team
spec:
  hard:
    # é€šç”¨å­˜å‚¨é…é¢
    requests.storage: "1Ti"       # æ€»å­˜å‚¨è¯·æ±‚é‡é™åˆ¶
    persistentvolumeclaims: "20"  # PVC æ•°é‡é™åˆ¶
    
    # æŒ‰å­˜å‚¨ç±»åˆ«çš„é…é¢
    fast-ssd.storageclass.storage.k8s.io/requests.storage: "500Gi"  # é«˜é€Ÿ SSD å­˜å‚¨é…é¢
    standard.storageclass.storage.k8s.io/requests.storage: "2Ti"    # æ ‡å‡†å­˜å‚¨é…é¢
    backup.storageclass.storage.k8s.io/requests.storage: "5Ti"     # å¤‡ä»½å­˜å‚¨é…é¢
    
    # æŒ‰å­˜å‚¨ç±»åˆ«çš„ PVC æ•°é‡
    fast-ssd.storageclass.storage.k8s.io/persistentvolumeclaims: "5"   # é«˜é€Ÿ SSD PVC æ•°é‡
    standard.storageclass.storage.k8s.io/persistentvolumeclaims: "10"  # æ ‡å‡† PVC æ•°é‡
    backup.storageclass.storage.k8s.io/persistentvolumeclaims: "20"    # å¤‡ä»½ PVC æ•°é‡

---
# é«˜é€Ÿå­˜å‚¨ PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fast-data-pvc
  namespace: data-processing
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi    # æ¶ˆè€— fast-ssd å­˜å‚¨é…é¢

---
# æ ‡å‡†å­˜å‚¨ PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: standard-data-pvc
  namespace: data-processing
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 500Gi    # æ¶ˆè€— standard å­˜å‚¨é…é¢

---
# å¤‡ä»½å­˜å‚¨ PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: data-processing
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: backup
  resources:
    requests:
      storage: 1Ti      # æ¶ˆè€— backup å­˜å‚¨é…é¢
```

## é«˜çº§é…ç½®

### 1. ä¼˜å…ˆçº§ç±»é…é¢

```yaml
# ä¼˜å…ˆçº§ç±»å®šä¹‰
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "é«˜ä¼˜å…ˆçº§å·¥ä½œè´Ÿè½½"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100
globalDefault: false
description: "ä½ä¼˜å…ˆçº§å·¥ä½œè´Ÿè½½"

---
# æŒ‰ä¼˜å…ˆçº§çš„èµ„æºé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: priority-quota
  namespace: mixed-workloads
  labels:
    quota-type: priority-based
spec:
  hard:
    # é«˜ä¼˜å…ˆçº§å·¥ä½œè´Ÿè½½é…é¢
    high-priority.priorityclass.scheduling.k8s.io/requests.cpu: "8"      # é«˜ä¼˜å…ˆçº§ CPU è¯·æ±‚
    high-priority.priorityclass.scheduling.k8s.io/requests.memory: 16Gi   # é«˜ä¼˜å…ˆçº§å†…å­˜è¯·æ±‚
    high-priority.priorityclass.scheduling.k8s.io/pods: "20"              # é«˜ä¼˜å…ˆçº§ Pod æ•°é‡
    
    # ä½ä¼˜å…ˆçº§å·¥ä½œè´Ÿè½½é…é¢
    low-priority.priorityclass.scheduling.k8s.io/requests.cpu: "4"        # ä½ä¼˜å…ˆçº§ CPU è¯·æ±‚
    low-priority.priorityclass.scheduling.k8s.io/requests.memory: 8Gi     # ä½ä¼˜å…ˆçº§å†…å­˜è¯·æ±‚
    low-priority.priorityclass.scheduling.k8s.io/pods: "50"               # ä½ä¼˜å…ˆçº§ Pod æ•°é‡
    
    # æ€»ä½“é…é¢ï¼ˆæ‰€æœ‰ä¼˜å…ˆçº§ï¼‰
    requests.cpu: "15"           # æ€» CPU è¯·æ±‚ï¼ˆé«˜ä¼˜å…ˆçº§ + ä½ä¼˜å…ˆçº§ + é»˜è®¤ï¼‰
    requests.memory: 30Gi        # æ€»å†…å­˜è¯·æ±‚
    pods: "100"                  # æ€» Pod æ•°é‡

---
# é«˜ä¼˜å…ˆçº§ Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
  namespace: mixed-workloads
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical-app
  template:
    metadata:
      labels:
        app: critical-app
    spec:
      priorityClassName: high-priority  # ä½¿ç”¨é«˜ä¼˜å…ˆçº§
      containers:
      - name: app
        image: critical-service:v1.0
        resources:
          requests:
            cpu: "1"        # æ¶ˆè€—é«˜ä¼˜å…ˆçº§ CPU é…é¢
            memory: 2Gi     # æ¶ˆè€—é«˜ä¼˜å…ˆçº§å†…å­˜é…é¢
          limits:
            cpu: "2"
            memory: 4Gi

---
# ä½ä¼˜å…ˆçº§ Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: batch-job
  namespace: mixed-workloads
spec:
  replicas: 5
  selector:
    matchLabels:
      app: batch-job
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      priorityClassName: low-priority   # ä½¿ç”¨ä½ä¼˜å…ˆçº§
      containers:
      - name: worker
        image: batch-worker:v1.0
        resources:
          requests:
            cpu: "200m"     # æ¶ˆè€—ä½ä¼˜å…ˆçº§ CPU é…é¢
            memory: 512Mi   # æ¶ˆè€—ä½ä¼˜å…ˆçº§å†…å­˜é…é¢
          limits:
            cpu: "500m"
            memory: 1Gi
```

### 2. ä½œç”¨åŸŸé…é¢

```yaml
# ç»ˆæ­¢çŠ¶æ€ Pod é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: terminating-quota
  namespace: batch-processing
  labels:
    quota-type: terminating
spec:
  # ä½œç”¨åŸŸï¼šåªå¯¹ç»ˆæ­¢çŠ¶æ€çš„ Pod ç”Ÿæ•ˆ
  scopes:
  - Terminating
  hard:
    requests.cpu: "5"        # ç»ˆæ­¢çŠ¶æ€ Pod çš„ CPU è¯·æ±‚æ€»é‡
    requests.memory: 10Gi    # ç»ˆæ­¢çŠ¶æ€ Pod çš„å†…å­˜è¯·æ±‚æ€»é‡
    pods: "20"               # ç»ˆæ­¢çŠ¶æ€ Pod çš„æ•°é‡

---
# éç»ˆæ­¢çŠ¶æ€ Pod é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: not-terminating-quota
  namespace: batch-processing
spec:
  # ä½œç”¨åŸŸï¼šåªå¯¹éç»ˆæ­¢çŠ¶æ€çš„ Pod ç”Ÿæ•ˆ
  scopes:
  - NotTerminating
  hard:
    requests.cpu: "20"       # éç»ˆæ­¢çŠ¶æ€ Pod çš„ CPU è¯·æ±‚æ€»é‡
    requests.memory: 40Gi    # éç»ˆæ­¢çŠ¶æ€ Pod çš„å†…å­˜è¯·æ±‚æ€»é‡
    pods: "50"               # éç»ˆæ­¢çŠ¶æ€ Pod çš„æ•°é‡

---
# BestEffort QoS ç±»é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-quota
  namespace: experimental
spec:
  # ä½œç”¨åŸŸï¼šåªå¯¹ BestEffort QoS ç±»çš„ Pod ç”Ÿæ•ˆ
  scopes:
  - BestEffort
  hard:
    pods: "10"               # BestEffort Pod æ•°é‡é™åˆ¶

---
# NotBestEffort QoS ç±»é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: not-besteffort-quota
  namespace: experimental
spec:
  # ä½œç”¨åŸŸï¼šåªå¯¹é BestEffort QoS ç±»çš„ Pod ç”Ÿæ•ˆ
  scopes:
  - NotBestEffort
  hard:
    requests.cpu: "10"       # é BestEffort Pod çš„ CPU è¯·æ±‚
    requests.memory: 20Gi    # é BestEffort Pod çš„å†…å­˜è¯·æ±‚
    pods: "30"               # é BestEffort Pod æ•°é‡
```

### 3. å¤šé…é¢ç»„åˆç­–ç•¥

```yaml
# åŸºç¡€è®¡ç®—èµ„æºé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: base-compute-quota
  namespace: enterprise-app
  labels:
    quota-category: compute
    priority: high
spec:
  hard:
    # åŸºç¡€è®¡ç®—èµ„æº
    requests.cpu: "50"           # æ€» CPU è¯·æ±‚
    limits.cpu: "100"            # æ€» CPU é™åˆ¶
    requests.memory: 100Gi       # æ€»å†…å­˜è¯·æ±‚
    limits.memory: 200Gi         # æ€»å†…å­˜é™åˆ¶
    requests.ephemeral-storage: 500Gi  # æ€»ä¸´æ—¶å­˜å‚¨è¯·æ±‚
    limits.ephemeral-storage: 1Ti      # æ€»ä¸´æ—¶å­˜å‚¨é™åˆ¶

---
# å¯¹è±¡æ•°é‡é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: enterprise-app
  labels:
    quota-category: objects
    priority: medium
spec:
  hard:
    # æ ¸å¿ƒå¯¹è±¡
    pods: "200"                  # Pod æ€»æ•°
    deployments.apps: "50"       # Deployment æ•°é‡
    services: "30"               # Service æ•°é‡
    configmaps: "100"            # ConfigMap æ•°é‡
    secrets: "50"                # Secret æ•°é‡
    
    # å­˜å‚¨å¯¹è±¡
    persistentvolumeclaims: "30" # PVC æ•°é‡
    
    # ç½‘ç»œå¯¹è±¡
    ingresses.networking.k8s.io: "20"  # Ingress æ•°é‡
    networkpolicies.networking.k8s.io: "10"  # NetworkPolicy æ•°é‡

---
# å­˜å‚¨ä¸“ç”¨é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: enterprise-app
  labels:
    quota-category: storage
    priority: high
spec:
  hard:
    # æ€»å­˜å‚¨é…é¢
    requests.storage: "10Ti"     # æ€»å­˜å‚¨è¯·æ±‚
    
    # æŒ‰å­˜å‚¨ç±»å‹åˆ†é…
    fast-ssd.storageclass.storage.k8s.io/requests.storage: "2Ti"    # é«˜é€Ÿå­˜å‚¨
    standard.storageclass.storage.k8s.io/requests.storage: "5Ti"   # æ ‡å‡†å­˜å‚¨
    archive.storageclass.storage.k8s.io/requests.storage: "20Ti"   # å½’æ¡£å­˜å‚¨
    
    # æŒ‰å­˜å‚¨ç±»å‹çš„ PVC æ•°é‡
    fast-ssd.storageclass.storage.k8s.io/persistentvolumeclaims: "10"
    standard.storageclass.storage.k8s.io/persistentvolumeclaims: "15"
    archive.storageclass.storage.k8s.io/persistentvolumeclaims: "5"

---
# GPU èµ„æºé…é¢ï¼ˆå¦‚æœé›†ç¾¤æ”¯æŒï¼‰
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
  namespace: enterprise-app
  labels:
    quota-category: gpu
    priority: critical
spec:
  hard:
    # GPU èµ„æº
    requests.nvidia.com/gpu: "20"    # GPU è¯·æ±‚æ€»æ•°
    limits.nvidia.com/gpu: "20"      # GPU é™åˆ¶æ€»æ•°
    
    # GPU å†…å­˜ï¼ˆå¦‚æœæ”¯æŒï¼‰
    requests.nvidia.com/gpu-memory: "160Gi"  # GPU å†…å­˜è¯·æ±‚
    limits.nvidia.com/gpu-memory: "160Gi"    # GPU å†…å­˜é™åˆ¶
```

## å®é™…åº”ç”¨åœºæ™¯

### 1. å¤šç§Ÿæˆ· SaaS å¹³å°

```yaml
# ä¼ä¸šçº§ç§Ÿæˆ·é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: enterprise-tenant-quota
  namespace: tenant-enterprise-001
  labels:
    tenant-id: enterprise-001
    plan: enterprise
    billing-tier: premium
  annotations:
    tenant-name: "Acme Corporation"
    contract-start: "2024-01-01"
    contract-end: "2024-12-31"
    contact: "admin@acme.com"
spec:
  hard:
    # è®¡ç®—èµ„æº - ä¼ä¸šçº§é…é¢
    requests.cpu: "100"          # 100 æ ¸ CPU è¯·æ±‚
    limits.cpu: "200"            # 200 æ ¸ CPU é™åˆ¶
    requests.memory: 400Gi       # 400GB å†…å­˜è¯·æ±‚
    limits.memory: 800Gi         # 800GB å†…å­˜é™åˆ¶
    
    # å¯¹è±¡æ•°é‡ - ä¼ä¸šçº§é™åˆ¶
    pods: "500"                  # 500 ä¸ª Pod
    deployments.apps: "100"      # 100 ä¸ª Deployment
    services: "50"               # 50 ä¸ª Service
    services.loadbalancers: "10" # 10 ä¸ª LoadBalancer
    ingresses.networking.k8s.io: "20"  # 20 ä¸ª Ingress
    
    # å­˜å‚¨èµ„æº - ä¼ä¸šçº§å­˜å‚¨
    requests.storage: "50Ti"     # 50TB å­˜å‚¨
    persistentvolumeclaims: "100" # 100 ä¸ª PVC
    
    # é…ç½®å’Œå¯†é’¥
    configmaps: "200"            # 200 ä¸ª ConfigMap
    secrets: "100"               # 100 ä¸ª Secret

---
# æ ‡å‡†ç§Ÿæˆ·é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: standard-tenant-quota
  namespace: tenant-standard-001
  labels:
    tenant-id: standard-001
    plan: standard
    billing-tier: medium
spec:
  hard:
    # è®¡ç®—èµ„æº - æ ‡å‡†é…é¢
    requests.cpu: "20"           # 20 æ ¸ CPU è¯·æ±‚
    limits.cpu: "40"             # 40 æ ¸ CPU é™åˆ¶
    requests.memory: 80Gi        # 80GB å†…å­˜è¯·æ±‚
    limits.memory: 160Gi         # 160GB å†…å­˜é™åˆ¶
    
    # å¯¹è±¡æ•°é‡ - æ ‡å‡†é™åˆ¶
    pods: "100"                  # 100 ä¸ª Pod
    deployments.apps: "20"       # 20 ä¸ª Deployment
    services: "15"               # 15 ä¸ª Service
    services.loadbalancers: "2"  # 2 ä¸ª LoadBalancer
    ingresses.networking.k8s.io: "5"   # 5 ä¸ª Ingress
    
    # å­˜å‚¨èµ„æº - æ ‡å‡†å­˜å‚¨
    requests.storage: "10Ti"     # 10TB å­˜å‚¨
    persistentvolumeclaims: "30" # 30 ä¸ª PVC
    
    # é…ç½®å’Œå¯†é’¥
    configmaps: "50"             # 50 ä¸ª ConfigMap
    secrets: "30"                # 30 ä¸ª Secret

---
# åŸºç¡€ç§Ÿæˆ·é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: basic-tenant-quota
  namespace: tenant-basic-001
  labels:
    tenant-id: basic-001
    plan: basic
    billing-tier: low
spec:
  hard:
    # è®¡ç®—èµ„æº - åŸºç¡€é…é¢
    requests.cpu: "5"            # 5 æ ¸ CPU è¯·æ±‚
    limits.cpu: "10"             # 10 æ ¸ CPU é™åˆ¶
    requests.memory: 20Gi        # 20GB å†…å­˜è¯·æ±‚
    limits.memory: 40Gi          # 40GB å†…å­˜é™åˆ¶
    
    # å¯¹è±¡æ•°é‡ - åŸºç¡€é™åˆ¶
    pods: "30"                   # 30 ä¸ª Pod
    deployments.apps: "10"       # 10 ä¸ª Deployment
    services: "5"                # 5 ä¸ª Service
    services.loadbalancers: "0"  # ä¸å…è®¸ LoadBalancer
    ingresses.networking.k8s.io: "2"   # 2 ä¸ª Ingress
    
    # å­˜å‚¨èµ„æº - åŸºç¡€å­˜å‚¨
    requests.storage: "1Ti"      # 1TB å­˜å‚¨
    persistentvolumeclaims: "10" # 10 ä¸ª PVC
    
    # é…ç½®å’Œå¯†é’¥
    configmaps: "20"             # 20 ä¸ª ConfigMap
    secrets: "10"                # 10 ä¸ª Secret
```

### 2. å¼€å‘ç¯å¢ƒèµ„æºç®¡ç†

```yaml
# å¼€å‘å›¢é˜Ÿ A é…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-team-a-quota
  namespace: dev-team-a
  labels:
    team: team-a
    environment: development
    cost-center: "engineering"
spec:
  hard:
    # å¼€å‘ç¯å¢ƒè®¡ç®—èµ„æº
    requests.cpu: "10"           # 10 æ ¸ CPU
    limits.cpu: "20"             # 20 æ ¸ CPUï¼ˆå…è®¸çªå‘ï¼‰
    requests.memory: 40Gi        # 40GB å†…å­˜
    limits.memory: 80Gi          # 80GB å†…å­˜
    
    # å¼€å‘ç¯å¢ƒå¯¹è±¡é™åˆ¶
    pods: "100"                  # 100 ä¸ª Pod
    deployments.apps: "30"       # 30 ä¸ª Deployment
    services: "20"               # 20 ä¸ª Service
    
    # å¼€å‘ç¯å¢ƒå­˜å‚¨
    requests.storage: "5Ti"      # 5TB å­˜å‚¨ï¼ˆåŒ…å«æµ‹è¯•æ•°æ®ï¼‰
    persistentvolumeclaims: "50" # 50 ä¸ª PVC
    
    # å¼€å‘ç¯å¢ƒé…ç½®
    configmaps: "100"            # 100 ä¸ª ConfigMapï¼ˆå„ç§é…ç½®ï¼‰
    secrets: "50"                # 50 ä¸ª Secret

---
# æµ‹è¯•ç¯å¢ƒé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: test-environment-quota
  namespace: testing
  labels:
    environment: testing
    purpose: integration-testing
spec:
  hard:
    # æµ‹è¯•ç¯å¢ƒéœ€è¦ç¨³å®šèµ„æº
    requests.cpu: "15"           # 15 æ ¸ CPU
    limits.cpu: "25"             # 25 æ ¸ CPU
    requests.memory: 60Gi        # 60GB å†…å­˜
    limits.memory: 100Gi         # 100GB å†…å­˜
    
    # æµ‹è¯•ç¯å¢ƒå¯¹è±¡
    pods: "150"                  # 150 ä¸ª Podï¼ˆå¹¶è¡Œæµ‹è¯•ï¼‰
    deployments.apps: "40"       # 40 ä¸ª Deployment
    services: "30"               # 30 ä¸ª Service
    jobs.batch: "50"             # 50 ä¸ªæµ‹è¯• Job
    
    # æµ‹è¯•æ•°æ®å­˜å‚¨
    requests.storage: "10Ti"     # 10TB å­˜å‚¨ï¼ˆæµ‹è¯•æ•°æ®ï¼‰
    persistentvolumeclaims: "30" # 30 ä¸ª PVC

---
# é¢„å‘å¸ƒç¯å¢ƒé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: staging-environment-quota
  namespace: staging
  labels:
    environment: staging
    purpose: pre-production
spec:
  hard:
    # é¢„å‘å¸ƒç¯å¢ƒæ¥è¿‘ç”Ÿäº§é…ç½®
    requests.cpu: "30"           # 30 æ ¸ CPU
    limits.cpu: "50"             # 50 æ ¸ CPU
    requests.memory: 120Gi       # 120GB å†…å­˜
    limits.memory: 200Gi         # 200GB å†…å­˜
    
    # é¢„å‘å¸ƒç¯å¢ƒå¯¹è±¡
    pods: "200"                  # 200 ä¸ª Pod
    deployments.apps: "50"       # 50 ä¸ª Deployment
    services: "40"               # 40 ä¸ª Service
    ingresses.networking.k8s.io: "15"  # 15 ä¸ª Ingress
    
    # é¢„å‘å¸ƒå­˜å‚¨
    requests.storage: "20Ti"     # 20TB å­˜å‚¨
    persistentvolumeclaims: "40" # 40 ä¸ª PVC
```

### 3. æœºå™¨å­¦ä¹ å¹³å°èµ„æºé…é¢

```yaml
# ML è®­ç»ƒç¯å¢ƒé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ml-training-quota
  namespace: ml-training
  labels:
    workload-type: ml-training
    resource-intensive: "true"
    gpu-enabled: "true"
spec:
  hard:
    # é«˜è®¡ç®—éœ€æ±‚
    requests.cpu: "200"          # 200 æ ¸ CPUï¼ˆå¤§è§„æ¨¡è®­ç»ƒï¼‰
    limits.cpu: "400"            # 400 æ ¸ CPU
    requests.memory: 1Ti         # 1TB å†…å­˜ï¼ˆå¤§æ¨¡å‹è®­ç»ƒï¼‰
    limits.memory: 2Ti           # 2TB å†…å­˜
    
    # GPU èµ„æº
    requests.nvidia.com/gpu: "50"    # 50 ä¸ª GPU
    limits.nvidia.com/gpu: "50"      # 50 ä¸ª GPU
    
    # è®­ç»ƒä»»åŠ¡å¯¹è±¡
    pods: "100"                  # 100 ä¸ªè®­ç»ƒ Pod
    jobs.batch: "200"            # 200 ä¸ªè®­ç»ƒ Job
    
    # å¤§é‡æ•°æ®å­˜å‚¨
    requests.storage: "500Ti"    # 500TB å­˜å‚¨ï¼ˆè®­ç»ƒæ•°æ®é›†ï¼‰
    persistentvolumeclaims: "100" # 100 ä¸ª PVC
    
    # æ¨¡å‹å’Œé…ç½®
    configmaps: "200"            # 200 ä¸ª ConfigMapï¼ˆè®­ç»ƒé…ç½®ï¼‰
    secrets: "100"               # 100 ä¸ª Secretï¼ˆAPI å¯†é’¥ç­‰ï¼‰

---
# ML æ¨ç†ç¯å¢ƒé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ml-inference-quota
  namespace: ml-inference
  labels:
    workload-type: ml-inference
    latency-sensitive: "true"
spec:
  hard:
    # æ¨ç†æœåŠ¡èµ„æº
    requests.cpu: "50"           # 50 æ ¸ CPU
    limits.cpu: "100"            # 100 æ ¸ CPU
    requests.memory: 200Gi       # 200GB å†…å­˜
    limits.memory: 400Gi         # 400GB å†…å­˜
    
    # æ¨ç† GPU
    requests.nvidia.com/gpu: "20"    # 20 ä¸ª GPU
    limits.nvidia.com/gpu: "20"      # 20 ä¸ª GPU
    
    # æ¨ç†æœåŠ¡å¯¹è±¡
    pods: "200"                  # 200 ä¸ªæ¨ç† Pod
    deployments.apps: "50"       # 50 ä¸ªæ¨ç† Deployment
    services: "50"               # 50 ä¸ªæ¨ç† Service
    ingresses.networking.k8s.io: "20"  # 20 ä¸ª Ingress
    
    # æ¨¡å‹å­˜å‚¨
    requests.storage: "50Ti"     # 50TB å­˜å‚¨ï¼ˆæ¨¡å‹æ–‡ä»¶ï¼‰
    persistentvolumeclaims: "50" # 50 ä¸ª PVC

---
# ML å®éªŒç¯å¢ƒé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ml-experiment-quota
  namespace: ml-experiments
  labels:
    workload-type: ml-experiments
    temporary: "true"
spec:
  hard:
    # å®éªŒç¯å¢ƒèµ„æº
    requests.cpu: "30"           # 30 æ ¸ CPU
    limits.cpu: "60"             # 60 æ ¸ CPU
    requests.memory: 120Gi       # 120GB å†…å­˜
    limits.memory: 240Gi         # 240GB å†…å­˜
    
    # å®éªŒ GPU
    requests.nvidia.com/gpu: "10"    # 10 ä¸ª GPU
    limits.nvidia.com/gpu: "10"      # 10 ä¸ª GPU
    
    # å®éªŒå¯¹è±¡ï¼ˆè¾ƒå¤šçŸ­æœŸä»»åŠ¡ï¼‰
    pods: "300"                  # 300 ä¸ªå®éªŒ Pod
    jobs.batch: "500"            # 500 ä¸ªå®éªŒ Job
    
    # å®éªŒæ•°æ®å­˜å‚¨
    requests.storage: "100Ti"    # 100TB å­˜å‚¨ï¼ˆå®éªŒæ•°æ®ï¼‰
    persistentvolumeclaims: "200" # 200 ä¸ª PVC
```

## å‘½ä»¤è¡Œæ“ä½œ

### åŸºæœ¬æ“ä½œ

```bash
# æŸ¥çœ‹ ResourceQuota
kubectl get resourcequotas
kubectl get resourcequota -n production
kubectl get quota  # ç®€å†™å½¢å¼

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
kubectl describe resourcequota compute-quota -n development
kubectl get resourcequota compute-quota -o yaml

# æŸ¥çœ‹æ‰€æœ‰å‘½åç©ºé—´çš„é…é¢
kubectl get resourcequotas --all-namespaces

# æŸ¥çœ‹é…é¢ä½¿ç”¨æƒ…å†µ
kubectl get resourcequota -o wide
```

### é…é¢çŠ¶æ€ç›‘æ§

```bash
# æŸ¥çœ‹é…é¢ä½¿ç”¨è¯¦æƒ…
kubectl describe resourcequota -n production

# æŸ¥çœ‹ç‰¹å®šé…é¢çš„ä½¿ç”¨æƒ…å†µ
kubectl get resourcequota compute-quota -n development -o jsonpath='{.status}' | jq .

# ç›‘æ§é…é¢ä½¿ç”¨å˜åŒ–
watch kubectl get resourcequota -n production

# æŸ¥çœ‹é…é¢ç›¸å…³äº‹ä»¶
kubectl get events --field-selector reason=FailedCreate -n production
```

### åˆ›å»ºå’Œç®¡ç†

```bash
# ä»æ–‡ä»¶åˆ›å»º ResourceQuota
kubectl apply -f resourcequota.yaml

# æ›´æ–° ResourceQuota
kubectl apply -f updated-resourcequota.yaml

# åˆ é™¤ ResourceQuota
kubectl delete resourcequota compute-quota -n development

# æ‰¹é‡åˆ é™¤
kubectl delete resourcequotas --all -n test-namespace
```

### é…é¢éªŒè¯å’Œæµ‹è¯•

```bash
# æµ‹è¯•é…é¢é™åˆ¶
# 1. å°è¯•åˆ›å»ºè¶…å‡ºé…é¢çš„èµ„æº
kubectl run test-pod --image=nginx --requests='cpu=100' -n development
# åº”è¯¥å¤±è´¥å¹¶æ˜¾ç¤ºé…é¢é”™è¯¯

# 2. æŸ¥çœ‹å½“å‰é…é¢ä½¿ç”¨æƒ…å†µ
kubectl describe resourcequota -n development

# 3. åˆ›å»ºç¬¦åˆé…é¢çš„èµ„æº
kubectl run test-pod --image=nginx --requests='cpu=1,memory=1Gi' -n development

# 4. å†æ¬¡æŸ¥çœ‹é…é¢ä½¿ç”¨æƒ…å†µ
kubectl describe resourcequota -n development
```

### é…é¢åˆ†æè„šæœ¬

```bash
#!/bin/bash
# é…é¢ä½¿ç”¨åˆ†æè„šæœ¬

NAMESPACE=${1:-default}

echo "=== ResourceQuota ä½¿ç”¨åˆ†æ - $NAMESPACE ==="

# æ£€æŸ¥æ˜¯å¦å­˜åœ¨ ResourceQuota
if ! kubectl get resourcequota -n $NAMESPACE &>/dev/null; then
    echo "å‘½åç©ºé—´ $NAMESPACE ä¸­æ²¡æœ‰ ResourceQuota"
    exit 0
fi

# è·å–æ‰€æœ‰ ResourceQuota
echo "1. ResourceQuota åˆ—è¡¨:"
kubectl get resourcequota -n $NAMESPACE
echo

# è¯¦ç»†ä½¿ç”¨æƒ…å†µ
echo "2. è¯¦ç»†ä½¿ç”¨æƒ…å†µ:"
for quota in $(kubectl get resourcequota -n $NAMESPACE -o jsonpath='{.items[*].metadata.name}'); do
    echo "--- $quota ---"
    kubectl describe resourcequota $quota -n $NAMESPACE | grep -A 20 "Resource\|Used\|Hard"
    echo
done

# è®¡ç®—ä½¿ç”¨ç‡
echo "3. ä½¿ç”¨ç‡åˆ†æ:"
kubectl get resourcequota -n $NAMESPACE -o json | jq -r '
  .items[] | 
  "\(.metadata.name):" as $name |
  .status.hard as $hard |
  .status.used as $used |
  $hard | keys[] as $resource |
  if $used[$resource] then
    ($used[$resource] | tonumber) / ($hard[$resource] | tonumber) * 100 as $percentage |
    "  \($resource): \($used[$resource])/\($hard[$resource]) (\($percentage | floor)%)"
  else
    "  \($resource): 0/\($hard[$resource]) (0%)"
  end
'

echo "=== åˆ†æå®Œæˆ ==="
```

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
| ---- | ---- | ---- |
| èµ„æºåˆ›å»ºå¤±è´¥ | è¶…å‡º ResourceQuota é™åˆ¶ | æ£€æŸ¥é…é¢ä½¿ç”¨æƒ…å†µï¼Œè°ƒæ•´é…é¢æˆ–åˆ é™¤ä¸éœ€è¦çš„èµ„æº |
| é…é¢æœªç”Ÿæ•ˆ | å‘½åç©ºé—´é”™è¯¯æˆ–é…é¢é…ç½®é”™è¯¯ | æ£€æŸ¥å‘½åç©ºé—´å’Œé…é¢é…ç½® |
| é…é¢è®¡ç®—é”™è¯¯ | èµ„æºå•ä½ä¸åŒ¹é… | æ£€æŸ¥èµ„æºå•ä½ï¼ˆCPUã€å†…å­˜ã€å­˜å‚¨ï¼‰ |
| æ— æ³•åˆ é™¤èµ„æº | é…é¢æ§åˆ¶å™¨é—®é¢˜ | æ£€æŸ¥ kube-controller-manager æ—¥å¿— |
| é…é¢çŠ¶æ€ä¸æ›´æ–° | æ§åˆ¶å™¨åŒæ­¥é—®é¢˜ | é‡å¯ç›¸å…³æ§åˆ¶å™¨æˆ–ç­‰å¾…åŒæ­¥ |

### è¯Šæ–­æ­¥éª¤

1. **æ£€æŸ¥é…é¢é…ç½®**
```bash
# ç¡®è®¤ ResourceQuota å­˜åœ¨ä¸”é…ç½®æ­£ç¡®
kubectl get resourcequota -n <namespace>
kubectl describe resourcequota <name> -n <namespace>
```

2. **æ£€æŸ¥é…é¢ä½¿ç”¨æƒ…å†µ**
```bash
# æŸ¥çœ‹å½“å‰ä½¿ç”¨æƒ…å†µ
kubectl describe resourcequota -n <namespace>

# æŸ¥çœ‹é…é¢çŠ¶æ€
kubectl get resourcequota -o yaml -n <namespace>
```

3. **æ£€æŸ¥å¤±è´¥çš„èµ„æºåˆ›å»º**
```bash
# æŸ¥çœ‹ç›¸å…³äº‹ä»¶
kubectl get events --field-selector reason=FailedCreate -n <namespace>

# æŸ¥çœ‹å…·ä½“é”™è¯¯ä¿¡æ¯
kubectl describe pod <pod-name> -n <namespace>
```

4. **éªŒè¯èµ„æºè®¡ç®—**
```bash
# æ‰‹åŠ¨è®¡ç®—èµ„æºä½¿ç”¨
kubectl get pods -n <namespace> -o json | jq '.items[].spec.containers[].resources.requests'
```

### å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

```yaml
# é”™è¯¯1ï¼šèµ„æºå•ä½ä¸ä¸€è‡´
# é”™è¯¯çš„é…ç½®
spec:
  hard:
    requests.memory: "10G"    # é”™è¯¯ï¼šåº”è¯¥ä½¿ç”¨ Gi
    requests.cpu: "10000m"    # å¯ä»¥ï¼Œä½†å»ºè®®ä½¿ç”¨ "10"

# æ­£ç¡®çš„é…ç½®
spec:
  hard:
    requests.memory: "10Gi"   # æ­£ç¡®ï¼šä½¿ç”¨ Gi
    requests.cpu: "10"        # æ­£ç¡®ï¼šä½¿ç”¨æ ¸æ•°

---
# é”™è¯¯2ï¼šé…é¢èŒƒå›´é…ç½®é”™è¯¯
# é”™è¯¯çš„é…ç½®
spec:
  scopes:
  - terminating           # é”™è¯¯ï¼šåº”è¯¥æ˜¯ Terminating
  hard:
    pods: "10"

# æ­£ç¡®çš„é…ç½®
spec:
  scopes:
  - Terminating           # æ­£ç¡®ï¼šé¦–å­—æ¯å¤§å†™
  hard:
    pods: "10"

---
# é”™è¯¯3ï¼šå­˜å‚¨ç±»é…é¢é…ç½®é”™è¯¯
# é”™è¯¯çš„é…ç½®
spec:
  hard:
    fast-ssd.storageclass.storage.k8s.io/request.storage: "100Gi"  # é”™è¯¯ï¼šrequest åº”è¯¥æ˜¯ requests

# æ­£ç¡®çš„é…ç½®
spec:
  hard:
    fast-ssd.storageclass.storage.k8s.io/requests.storage: "100Gi"  # æ­£ç¡®ï¼šä½¿ç”¨ requests
```

## æœ€ä½³å®è·µ

### 1. é…é¢è®¾è®¡åŸåˆ™

```yaml
# 1. åˆ†å±‚é…é¢è®¾è®¡
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-base-quota
  namespace: production
  labels:
    quota-tier: base
    environment: production
    review-cycle: quarterly
  annotations:
    description: "ç”Ÿäº§ç¯å¢ƒåŸºç¡€èµ„æºé…é¢"
    owner: "platform-team@company.com"
    last-review: "2024-01-15"
    next-review: "2024-04-15"
    escalation-contact: "sre-team@company.com"
spec:
  hard:
    # ä¿å®ˆçš„åŸºç¡€é…é¢
    requests.cpu: "50"           # åŸºç¡€ CPU é…é¢
    limits.cpu: "100"            # åŸºç¡€ CPU é™åˆ¶
    requests.memory: 200Gi       # åŸºç¡€å†…å­˜é…é¢
    limits.memory: 400Gi         # åŸºç¡€å†…å­˜é™åˆ¶
    
    # åŸºç¡€å¯¹è±¡æ•°é‡
    pods: "200"                  # åŸºç¡€ Pod æ•°é‡
    deployments.apps: "50"       # åŸºç¡€ Deployment æ•°é‡
    services: "30"               # åŸºç¡€ Service æ•°é‡
    
    # åŸºç¡€å­˜å‚¨
    requests.storage: "50Ti"     # åŸºç¡€å­˜å‚¨é…é¢
    persistentvolumeclaims: "100" # åŸºç¡€ PVC æ•°é‡

---
# 2. æ¸è¿›å¼é…é¢å¢é•¿
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-extended-quota
  namespace: production
  labels:
    quota-tier: extended
    environment: production
    approval-required: "true"
spec:
  hard:
    # æ‰©å±•é…é¢ï¼ˆéœ€è¦å®¡æ‰¹ï¼‰
    requests.cpu: "100"          # æ‰©å±• CPU é…é¢
    limits.cpu: "200"            # æ‰©å±• CPU é™åˆ¶
    requests.memory: 500Gi       # æ‰©å±•å†…å­˜é…é¢
    limits.memory: 1Ti           # æ‰©å±•å†…å­˜é™åˆ¶
    
    # æ‰©å±•å¯¹è±¡æ•°é‡
    pods: "500"                  # æ‰©å±• Pod æ•°é‡
    deployments.apps: "100"      # æ‰©å±• Deployment æ•°é‡
    services: "80"               # æ‰©å±• Service æ•°é‡
    
    # æ‰©å±•å­˜å‚¨
    requests.storage: "200Ti"    # æ‰©å±•å­˜å‚¨é…é¢
    persistentvolumeclaims: "300" # æ‰©å±• PVC æ•°é‡
```

### 2. ç›‘æ§å’Œå‘Šè­¦

```yaml
# Prometheus ç›‘æ§è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: resourcequota-monitoring
  namespace: monitoring
spec:
  groups:
  - name: resourcequota.rules
    rules:
    # é…é¢ä½¿ç”¨ç‡ç›‘æ§
    - record: kubernetes:resourcequota:usage_ratio
      expr: |
        (
          kube_resourcequota{type="used"} 
          / 
          kube_resourcequota{type="hard"}
        ) * 100
    
    # CPU é…é¢ä½¿ç”¨ç‡å‘Šè­¦
    - alert: ResourceQuotaCPUUsageHigh
      expr: |
        (
          kube_resourcequota{resource="requests.cpu", type="used"} 
          / 
          kube_resourcequota{resource="requests.cpu", type="hard"}
        ) * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "å‘½åç©ºé—´ CPU é…é¢ä½¿ç”¨ç‡è¿‡é«˜"
        description: "å‘½åç©ºé—´ {{ $labels.namespace }} çš„ CPU é…é¢ä½¿ç”¨ç‡ä¸º {{ $value | humanizePercentage }}"
    
    # å†…å­˜é…é¢ä½¿ç”¨ç‡å‘Šè­¦
    - alert: ResourceQuotaMemoryUsageHigh
      expr: |
        (
          kube_resourcequota{resource="requests.memory", type="used"} 
          / 
          kube_resourcequota{resource="requests.memory", type="hard"}
        ) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "å‘½åç©ºé—´å†…å­˜é…é¢ä½¿ç”¨ç‡è¿‡é«˜"
        description: "å‘½åç©ºé—´ {{ $labels.namespace }} çš„å†…å­˜é…é¢ä½¿ç”¨ç‡ä¸º {{ $value | humanizePercentage }}"
    
    # é…é¢å³å°†è€—å°½å‘Šè­¦
    - alert: ResourceQuotaNearExhaustion
      expr: |
        (
          kube_resourcequota{type="used"} 
          / 
          kube_resourcequota{type="hard"}
        ) * 100 > 95
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "èµ„æºé…é¢å³å°†è€—å°½"
        description: "å‘½åç©ºé—´ {{ $labels.namespace }} çš„ {{ $labels.resource }} é…é¢ä½¿ç”¨ç‡ä¸º {{ $value | humanizePercentage }}ï¼Œå³å°†è€—å°½"
    
    # Pod æ•°é‡é…é¢å‘Šè­¦
    - alert: ResourceQuotaPodCountHigh
      expr: |
        (
          kube_resourcequota{resource="pods", type="used"} 
          / 
          kube_resourcequota{resource="pods", type="hard"}
        ) * 100 > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod æ•°é‡é…é¢ä½¿ç”¨ç‡è¿‡é«˜"
        description: "å‘½åç©ºé—´ {{ $labels.namespace }} çš„ Pod æ•°é‡é…é¢ä½¿ç”¨ç‡ä¸º {{ $value | humanizePercentage }}"
```

### 3. è‡ªåŠ¨åŒ–é…é¢ç®¡ç†

```bash
#!/bin/bash
# è‡ªåŠ¨é…é¢ç®¡ç†è„šæœ¬

set -e

# é…ç½®
CONFIG_FILE="/etc/kubernetes/quota-config.yaml"
LOG_FILE="/var/log/quota-manager.log"
SLACK_WEBHOOK="${SLACK_WEBHOOK_URL}"

# æ—¥å¿—å‡½æ•°
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

# å‘é€ Slack é€šçŸ¥
send_slack_notification() {
    local message="$1"
    local color="$2"
    
    if [[ -n "$SLACK_WEBHOOK" ]]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"attachments\":[{\"color\":\"$color\",\"text\":\"$message\"}]}" \
            $SLACK_WEBHOOK
    fi
}

# æ£€æŸ¥é…é¢ä½¿ç”¨ç‡
check_quota_usage() {
    local namespace="$1"
    local threshold="${2:-80}"
    
    log "æ£€æŸ¥å‘½åç©ºé—´ $namespace çš„é…é¢ä½¿ç”¨æƒ…å†µ"
    
    # è·å–é…é¢ä½¿ç”¨æƒ…å†µ
    local quota_data=$(kubectl get resourcequota -n $namespace -o json 2>/dev/null)
    
    if [[ -z "$quota_data" ]]; then
        log "å‘½åç©ºé—´ $namespace ä¸­æ²¡æœ‰ ResourceQuota"
        return 0
    fi
    
    # åˆ†ææ¯ä¸ªé…é¢é¡¹
    echo "$quota_data" | jq -r '
        .items[] |
        .metadata.name as $quota_name |
        .status.hard as $hard |
        .status.used as $used |
        $hard | keys[] as $resource |
        if $used[$resource] then
            ($used[$resource] | tonumber) / ($hard[$resource] | tonumber) * 100 as $percentage |
            if $percentage > '$threshold' then
                "WARNING: \($quota_name)/\($resource): \($percentage | floor)% (\($used[$resource])/\($hard[$resource]))"
            else
                "OK: \($quota_name)/\($resource): \($percentage | floor)% (\($used[$resource])/\($hard[$resource]))"
            end
        else
            "OK: \($quota_name)/\($resource): 0% (0/\($hard[$resource]))"
        end
    ' | while read line; do
        if [[ $line == WARNING:* ]]; then
            log "$line"
            send_slack_notification "ğŸš¨ é…é¢å‘Šè­¦: $namespace - $line" "warning"
        else
            log "$line"
        fi
    done
}

# è‡ªåŠ¨æ‰©å±•é…é¢
auto_scale_quota() {
    local namespace="$1"
    local resource="$2"
    local current_usage="$3"
    local current_limit="$4"
    local usage_percentage="$5"
    
    # å¦‚æœä½¿ç”¨ç‡è¶…è¿‡ 90%ï¼Œè‡ªåŠ¨æ‰©å±• 20%
    if (( $(echo "$usage_percentage > 90" | bc -l) )); then
        local new_limit=$(echo "$current_limit * 1.2" | bc -l | cut -d. -f1)
        
        log "è‡ªåŠ¨æ‰©å±•é…é¢: $namespace/$resource ä» $current_limit æ‰©å±•åˆ° $new_limit"
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨é…é¢æ›´æ–° API æˆ–ç”Ÿæˆé…é¢æ›´æ–°è¯·æ±‚
        # kubectl patch resourcequota ... (éœ€è¦å…·ä½“å®ç°)
        
        send_slack_notification "ğŸ“ˆ è‡ªåŠ¨é…é¢æ‰©å±•: $namespace/$resource $current_limit â†’ $new_limit" "good"
    fi
}

# ç”Ÿæˆé…é¢æŠ¥å‘Š
generate_quota_report() {
    local output_file="/tmp/quota-report-$(date +%Y%m%d).html"
    
    log "ç”Ÿæˆé…é¢ä½¿ç”¨æŠ¥å‘Š: $output_file"
    
    cat > $output_file << 'EOF'
<!DOCTYPE html>
<html>
<head>
    <title>Kubernetes ResourceQuota ä½¿ç”¨æŠ¥å‘Š</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .warning { background-color: #fff3cd; }
        .critical { background-color: #f8d7da; }
        .ok { background-color: #d4edda; }
    </style>
</head>
<body>
    <h1>Kubernetes ResourceQuota ä½¿ç”¨æŠ¥å‘Š</h1>
    <p>ç”Ÿæˆæ—¶é—´: $(date)</p>
    <table>
        <tr>
            <th>å‘½åç©ºé—´</th>
            <th>é…é¢åç§°</th>
            <th>èµ„æºç±»å‹</th>
            <th>å·²ä½¿ç”¨</th>
            <th>æ€»é…é¢</th>
            <th>ä½¿ç”¨ç‡</th>
            <th>çŠ¶æ€</th>
        </tr>
EOF
    
    # è·å–æ‰€æœ‰å‘½åç©ºé—´çš„é…é¢ä¿¡æ¯
    for namespace in $(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}'); do
        kubectl get resourcequota -n $namespace -o json 2>/dev/null | jq -r '
            .items[] |
            .metadata.name as $quota_name |
            .metadata.namespace as $ns |
            .status.hard as $hard |
            .status.used as $used |
            $hard | keys[] as $resource |
            if $used[$resource] then
                ($used[$resource] | tonumber) / ($hard[$resource] | tonumber) * 100 as $percentage |
                if $percentage > 90 then
                    "        <tr class=\"critical\"><td>\($ns)</td><td>\($quota_name)</td><td>\($resource)</td><td>\($used[$resource])</td><td>\($hard[$resource])</td><td>\($percentage | floor)%</td><td>å±é™©</td></tr>"
                elif $percentage > 80 then
                    "        <tr class=\"warning\"><td>\($ns)</td><td>\($quota_name)</td><td>\($resource)</td><td>\($used[$resource])</td><td>\($hard[$resource])</td><td>\($percentage | floor)%</td><td>è­¦å‘Š</td></tr>"
                else
                    "        <tr class=\"ok\"><td>\($ns)</td><td>\($quota_name)</td><td>\($resource)</td><td>\($used[$resource])</td><td>\($hard[$resource])</td><td>\($percentage | floor)%</td><td>æ­£å¸¸</td></tr>"
                end
            else
                "        <tr class=\"ok\"><td>\($ns)</td><td>\($quota_name)</td><td>\($resource)</td><td>0</td><td>\($hard[$resource])</td><td>0%</td><td>æ­£å¸¸</td></tr>"
            end
        ' >> $output_file
    done
    
    cat >> $output_file << 'EOF'
    </table>
</body>
</html>
EOF
    
    log "é…é¢æŠ¥å‘Šå·²ç”Ÿæˆ: $output_file"
}

# ä¸»å‡½æ•°
main() {
    log "å¼€å§‹é…é¢ç®¡ç†ä»»åŠ¡"
    
    # æ£€æŸ¥æ‰€æœ‰å‘½åç©ºé—´çš„é…é¢ä½¿ç”¨æƒ…å†µ
    for namespace in $(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}'); do
        check_quota_usage $namespace 80
    done
    
    # ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
    if [[ $(date +%H) == "09" ]]; then  # æ¯å¤©ä¸Šåˆ 9 ç‚¹ç”ŸæˆæŠ¥å‘Š
        generate_quota_report
    fi
    
    log "é…é¢ç®¡ç†ä»»åŠ¡å®Œæˆ"
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
```

## æ€»ç»“

ResourceQuota æ˜¯ Kubernetes ä¸­é‡è¦çš„èµ„æºæ²»ç†å·¥å…·ï¼Œå®ƒæä¾›äº†å‘½åç©ºé—´çº§åˆ«çš„èµ„æºæ€»é‡æ§åˆ¶ï¼Œæ˜¯å¤šç§Ÿæˆ·ç¯å¢ƒå’Œèµ„æºç®¡ç†çš„æ ¸å¿ƒç»„ä»¶ã€‚

**å…³é”®è¦ç‚¹**ï¼š
- ResourceQuota æ§åˆ¶å‘½åç©ºé—´å†…èµ„æºçš„æ€»ä½“ä½¿ç”¨é‡ï¼ŒåŒ…æ‹¬è®¡ç®—èµ„æºã€å­˜å‚¨èµ„æºå’Œå¯¹è±¡æ•°é‡
- æ”¯æŒå¤šç§é…é¢ç±»å‹å’Œä½œç”¨åŸŸï¼Œå¯ä»¥ç²¾ç¡®æ§åˆ¶ä¸åŒç±»å‹èµ„æºçš„ä½¿ç”¨
- é€šè¿‡å‡†å…¥æ§åˆ¶å™¨å®æ—¶æ£€æŸ¥å’Œæ›´æ–°é…é¢ä½¿ç”¨æƒ…å†µ
- æ˜¯å¤šç§Ÿæˆ·ç¯å¢ƒèµ„æºéš”ç¦»å’Œå…¬å¹³åˆ†é…çš„é‡è¦ä¿éšœ
- éœ€è¦ç»“åˆç›‘æ§å‘Šè­¦å’Œè‡ªåŠ¨åŒ–ç®¡ç†æ¥ç¡®ä¿é…é¢ç­–ç•¥çš„æœ‰æ•ˆæ‰§è¡Œ
- åº”è¯¥æ ¹æ®ä¸šåŠ¡éœ€æ±‚å’Œç¯å¢ƒç‰¹ç‚¹åˆ¶å®šåˆé€‚çš„é…é¢ç­–ç•¥